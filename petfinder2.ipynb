{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3YDQH4n9zdG"
   },
   "source": [
    "# üìî About this notebook\n",
    "\n",
    "PetFinder.my - Pawpularity Contest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK0pnACt-CQd",
    "tags": []
   },
   "source": [
    "## üìù Memo\n",
    "\n",
    "- ensemble\n",
    "    - seed\n",
    "    - fold\n",
    "    - size\n",
    "    - base, large\n",
    "    - other models\n",
    "        - CSWin\n",
    "        - EffNet\n",
    "        - NFNet\n",
    "        - [CaiT](https://github.com/facebookresearch/deit/blob/main/README_cait.md)\n",
    "        - [ConvMixer](https://github.com/tmp-iclr/convmixer)\n",
    "        - +SVR with feats\n",
    "        - hybrid swin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eezrDVYa_yav"
   },
   "source": [
    "# üìö Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3282,
     "status": "ok",
     "timestamp": 1633394791716,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "32GYVF3l_wUO"
   },
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import collections\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import statistics\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1153,
     "status": "ok",
     "timestamp": 1633394792859,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "DhlpmaPbNqOs"
   },
   "outputs": [],
   "source": [
    "# Competition specific libraries\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import timm\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from nfnets.agc import AGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional local libraries\n",
    "\n",
    "# https://github.com/microsoft/CSWin-Transformer\n",
    "sys.path.append(\"../input/CSWin-Transformer\")\n",
    "import models\n",
    "\n",
    "# https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch\n",
    "sys.path.append(\"../input/Yet-Another-EfficientDet-Pytorch\")\n",
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import (\n",
    "    STANDARD_COLORS,\n",
    "    get_index_label,\n",
    "    invert_affine,\n",
    "    plot_one_box,\n",
    "    postprocess,\n",
    "    preprocess,\n",
    "    standard_to_bgr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1633394792860,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "qPMtyPp8AWvZ"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2322,
     "status": "ok",
     "timestamp": 1633394795177,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "ZHMIcuPvAdZI",
    "outputId": "8b28afb5-3466-40d4-9d0f-7a7a43f786b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimokuri\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_CACHE_DIR\"] = \"/data/home/sugiyama/.cache/wandb\"\n",
    "\n",
    "# netrc = \"/content/drive/MyDrive/.netrc\" if 'google.colab' in sys.modules else \"../input/wandbtoken/.netrc\"\n",
    "# !cp -f {netrc} ~/\n",
    "!wandb login\n",
    "wandb_tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1633394795177,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "KPb3f6kLAYrP"
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÜçÁèæÊÄß„Åå„Å™„Åè„Å™„Å£„Åü„Å®„Åç„ÅØ False „Å´„Åô„Çã\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9uOtCk2BKGF"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1633394795178,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "m4LaDlPJBLil"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input/petfinder-pawpularity-score/\"\n",
    "OUTPUT_DIR = \"./\"\n",
    "MODEL_DIR = \"./models/\"\n",
    "\n",
    "!rm -rf {MODEL_DIR}\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1633394795178,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "xKKkrScBBf44"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
    "sub = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n",
    "\n",
    "TRAIN_IMAGE_PATH = DATA_DIR + \"train/\"\n",
    "TEST_IMAGE_PATH = DATA_DIR + \"test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQk8_n-5Bwun"
   },
   "source": [
    "# üõ† Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1633394795179,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "HfK7G4EQBxpt",
    "outputId": "266be0c6-6b79-4de4-8de1-8502eb577dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n"
     ]
    }
   ],
   "source": [
    "# seed = random.randrange(10000)\n",
    "seed = 440\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633394795179,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "STqXGbUhBz5f"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    wandb_entity = \"imokuri\"\n",
    "    wandb_project = \"petfinder2\"\n",
    "    print_freq = 100\n",
    "\n",
    "    preprocess = False  # crop images\n",
    "    train = True\n",
    "    train2 = False  # SVR\n",
    "    train3 = False  # 100 or not\n",
    "    validate = False\n",
    "    inference = False\n",
    "\n",
    "    debug = False\n",
    "    num_debug_data = 1000\n",
    "\n",
    "    amp = True\n",
    "    multi_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAK99CZOhfMV"
   },
   "source": [
    "Model examples\n",
    "\n",
    "- resnet200d\n",
    "- resnext50_32x4d\n",
    "- ‚òÖdm_nfnet_f3\n",
    "- vit_base_patch16_384\n",
    "- tf_efficientnet_b7_ns\n",
    "- tf_efficientnetv2_l_in21k\n",
    "- swin_base_patch4_window12_384_in22k\n",
    "- swin_large_patch4_window7_224_in22k\n",
    "- ‚òÖswin_large_patch4_window12_384_in22k\n",
    "- CSWin_144_24322_large_224\n",
    "- CSWin_144_24322_large_384\n",
    "- convmixer_1536_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1633394795180,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "Vjl7MQHiFyFS"
   },
   "outputs": [],
   "source": [
    "config_defaults = {\n",
    "    \"seed\": seed,\n",
    "    \"n_class\": 1,\n",
    "    \"n_fold\": 10,\n",
    "    \"epochs\": 10,\n",
    "    \"es_patience\": 0,\n",
    "    \"batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"max_grad_norm\": 1000,\n",
    "    # \"agc_clipping\": 1e-3,\n",
    "    \"criterion\": \"BCEWithLogitsLoss\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"CosineAnnealingWarmRestarts\",\n",
    "    \"lr\": 5e-6,\n",
    "    \"min_lr\": 5e-6,\n",
    "    \"weight_decay\": 1e-7,\n",
    "    # \"momentum\": 0.9,\n",
    "    \"model_name\": \"dm_nfnet_f2\",\n",
    "    \"model_embedder\": \"\",\n",
    "    \"size\": 352,\n",
    "    # \"compound_coef\": 6,\n",
    "    \"models\": [\n",
    "        # \"swin_large_patch4_window12_384_in22k:v14\",\n",
    "        # \"swin_base_patch4_window12_384_in22k:v1\",\n",
    "    ],\n",
    "    \"runs\": [\n",
    "        # \"ymawjqn2\",  # 71 - swin large 384\n",
    "        # \"tppuuj4q\",  # 72 - swin large 224\n",
    "        # \"3ptw9dio\",  # 73 - swin base 384\n",
    "        # \"bulu150s\",  # 74 - swin base 224\n",
    "        # \"lxvhq95s\",  # 99 - nfnet f3 416\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633394795180,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "dgConxiYIqTM"
   },
   "outputs": [],
   "source": [
    "if Config.debug:\n",
    "    config_defaults[\"n_fold\"] = 3\n",
    "    config_defaults[\"epochs\"] = 1\n",
    "    Config.print_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1633394795181,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "BteR9_gVItes"
   },
   "outputs": [],
   "source": [
    "if Config.train:\n",
    "    wandb_job_type = \"training\"\n",
    "\n",
    "elif Config.train2:\n",
    "    wandb_job_type = \"training2\"\n",
    "\n",
    "elif Config.train3:\n",
    "    wandb_job_type = \"training3\"\n",
    "\n",
    "elif Config.preprocess:\n",
    "    wandb_job_type = \"preprocess\"\n",
    "\n",
    "elif Config.inference:\n",
    "    wandb_job_type = \"inference\"\n",
    "\n",
    "elif Config.validate:\n",
    "    wandb_job_type = \"validation\"\n",
    "\n",
    "else:\n",
    "    wandb_job_type = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633394795181,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "LIDKN-b7jDOt"
   },
   "outputs": [],
   "source": [
    "wandb_tags.append(\"basic aug\")\n",
    "# wandb_tags.append(\"heavy aug\")\n",
    "# wandb_tags.append(\"mixup\")\n",
    "# wandb_tags.append(\"cutmix\")\n",
    "# wandb_tags.append(\"freeze norm\")  # for only efficientnet\n",
    "# wandb_tags.append(\"crop image\")\n",
    "# wandb_tags.append(\"tune lr\")  # Run only single fold\n",
    "# wandb_tags.append(\"agc\")  # Adaptive Gradient Clipping for only nfnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"petfinder2.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1612,
     "status": "ok",
     "timestamp": 1633394796778,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "JaQsAlSfJbnt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimokuri\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/imokuri/petfinder2/runs/3g9myvii\" target=\"_blank\">revived-bird-141</a></strong> to <a href=\"https://wandb.ai/imokuri/petfinder2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if Config.debug or Config.preprocess:\n",
    "    run = wandb.init(\n",
    "        entity=Config.wandb_entity,\n",
    "        project=Config.wandb_project,\n",
    "        config=config_defaults,\n",
    "        tags=wandb_tags,\n",
    "        mode=\"disabled\",\n",
    "    )\n",
    "else:\n",
    "    run = wandb.init(\n",
    "        entity=Config.wandb_entity,\n",
    "        project=Config.wandb_project,\n",
    "        config=config_defaults,\n",
    "        job_type=wandb_job_type,\n",
    "        tags=wandb_tags,\n",
    "        save_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1633394796783,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "0kD8_kP0JjAC"
   },
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYjWTUW9Jtem"
   },
   "source": [
    "# EDA 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1633394796784,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "-dEUJ_P9Jwzw",
    "outputId": "f98c8f40-5d7e-43f5-f524-58129cd3c758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9912 entries, 0 to 9911\n",
      "Data columns (total 14 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Id             9912 non-null   object\n",
      " 1   Subject Focus  9912 non-null   int64 \n",
      " 2   Eyes           9912 non-null   int64 \n",
      " 3   Face           9912 non-null   int64 \n",
      " 4   Near           9912 non-null   int64 \n",
      " 5   Action         9912 non-null   int64 \n",
      " 6   Accessory      9912 non-null   int64 \n",
      " 7   Group          9912 non-null   int64 \n",
      " 8   Collage        9912 non-null   int64 \n",
      " 9   Human          9912 non-null   int64 \n",
      " 10  Occlusion      9912 non-null   int64 \n",
      " 11  Info           9912 non-null   int64 \n",
      " 12  Blur           9912 non-null   int64 \n",
      " 13  Pawpularity    9912 non-null   int64 \n",
      "dtypes: int64(13), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \n",
       "0          0      1        0      0          0     0     0           63  \n",
       "1          0      0        0      0          0     0     0           42  \n",
       "2          0      0        0      1          1     0     0           28  \n",
       "3          0      0        0      0          0     0     0           15  \n",
       "4          0      1        0      0          0     0     0           72  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Id             8 non-null      object\n",
      " 1   Subject Focus  8 non-null      int64 \n",
      " 2   Eyes           8 non-null      int64 \n",
      " 3   Face           8 non-null      int64 \n",
      " 4   Near           8 non-null      int64 \n",
      " 5   Action         8 non-null      int64 \n",
      " 6   Accessory      8 non-null      int64 \n",
      " 7   Group          8 non-null      int64 \n",
      " 8   Collage        8 non-null      int64 \n",
      " 9   Human          8 non-null      int64 \n",
      " 10  Occlusion      8 non-null      int64 \n",
      " 11  Info           8 non-null      int64 \n",
      " 12  Blur           8 non-null      int64 \n",
      "dtypes: int64(12), object(1)\n",
      "memory usage: 960.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4128bae22183829d2b5fea10effdb0c3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43a2262d7738e3d420d453815151079e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4e429cead1848a298432a0acad014c9d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80bc3ccafcc51b66303c2c263aa38486</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8f49844c382931444e68dffbe20228f4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  4128bae22183829d2b5fea10effdb0c3              1     0     1     0       0   \n",
       "1  43a2262d7738e3d420d453815151079e              0     1     0     0       0   \n",
       "2  4e429cead1848a298432a0acad014c9d              0     0     0     1       0   \n",
       "3  80bc3ccafcc51b66303c2c263aa38486              1     0     1     0       0   \n",
       "4  8f49844c382931444e68dffbe20228f4              1     1     1     0       1   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  \n",
       "0          1      1        0      0          1     0     1  \n",
       "1          0      1        1      0          0     0     0  \n",
       "2          1      1        1      0          1     1     1  \n",
       "3          0      0        0      0          0     1     0  \n",
       "4          1      0        1      0          1     1     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Id           8 non-null      object \n",
      " 1   Pawpularity  8 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 256.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4128bae22183829d2b5fea10effdb0c3</td>\n",
       "      <td>67.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43a2262d7738e3d420d453815151079e</td>\n",
       "      <td>59.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4e429cead1848a298432a0acad014c9d</td>\n",
       "      <td>20.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80bc3ccafcc51b66303c2c263aa38486</td>\n",
       "      <td>94.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8f49844c382931444e68dffbe20228f4</td>\n",
       "      <td>89.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Pawpularity\n",
       "0  4128bae22183829d2b5fea10effdb0c3        67.75\n",
       "1  43a2262d7738e3d420d453815151079e        59.15\n",
       "2  4e429cead1848a298432a0acad014c9d        20.02\n",
       "3  80bc3ccafcc51b66303c2c263aa38486        94.53\n",
       "4  8f49844c382931444e68dffbe20228f4        89.82"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for df in [train, test, sub]:\n",
    "    print(f\"=\" * 120)\n",
    "    df.info()\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1633394797327,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "uK-vChyCKVhZ",
    "outputId": "3d5cf17f-3df5-4163-a7ba-be4bd83f6211"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Pawpularity'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(train[\"Pawpularity\"], kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nN20A-4KeX2"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess3(df):\n",
    "    df[\"Pawpularity\"] = np.where(df[\"Pawpularity\"] == 100, 1, 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.train3:\n",
    "    train = train_preprocess3(train)\n",
    "    print(train[\"Pawpularity\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kDt-OBGKg33"
   },
   "source": [
    "# EDA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633394797329,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "m2XP3TB7KhvW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFYIILAsKih4"
   },
   "source": [
    "# üëë Load Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.train2:\n",
    "    api = wandb.Api()\n",
    "    for artifact_id in config.models:\n",
    "        name_version = artifact_id.replace(\":\", \"-\")\n",
    "        if not os.path.exists(name_version):\n",
    "            os.makedirs(name_version)\n",
    "\n",
    "        try:\n",
    "            artifact_path = f\"{Config.wandb_entity}/{Config.wandb_project}/{artifact_id}\"\n",
    "            artifact = api.artifact(artifact_path)\n",
    "            artifact.download(name_version)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {artifact_path}, {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.validate:\n",
    "    api = wandb.Api()\n",
    "\n",
    "    for n, run_id in enumerate(config.runs):\n",
    "        if not os.path.exists(run_id):\n",
    "            os.makedirs(run_id)\n",
    "\n",
    "        run_path = f\"{Config.wandb_entity}/{Config.wandb_project}/{run_id}\"\n",
    "        run = api.run(run_path)\n",
    "\n",
    "        try:\n",
    "            run.file(\"oof_df.csv\").download(run_id)\n",
    "        except wandb.CommError:\n",
    "            # Already downloaded.\n",
    "            pass\n",
    "\n",
    "        oof = pd.read_csv(f\"{run_id}/oof_df.csv\")[[\"Id\", \"preds\"]]\n",
    "        oof.columns = [\"Id\", f\"preds{n}\"]\n",
    "        train = pd.merge(train, oof, on=\"Id\")\n",
    "\n",
    "    print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633394797329,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "ahzUGk4sK6te"
   },
   "outputs": [],
   "source": [
    "if Config.debug:\n",
    "    train = train.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
    "    if len(sub) > Config.num_debug_data:\n",
    "        test = test.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
    "        sub = sub.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF4pvz7-L7qT"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1633394797329,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "qrlmhizKMCku"
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f\"[{name}] start\")\n",
    "    yield\n",
    "    LOGGER.info(f\"[{name}] done in {time.time() - t0:.0f} s.\")\n",
    "\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_torch(seed=config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClI1iqtWLqU0"
   },
   "source": [
    "# Make Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633394797330,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "BQVW6__Rjk_N"
   },
   "outputs": [],
   "source": [
    "num_bins = int(np.floor(1 + np.log2(len(train))))\n",
    "train.loc[:, \"bins\"] = pd.cut(train[\"Pawpularity\"], bins=num_bins, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1633394797330,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "raBuQ3EjLsmo",
    "outputId": "11fbcaa3-b48c-4143-c742-b0de3e717a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  bins\n",
      "0     0        33\n",
      "      1        42\n",
      "      2       111\n",
      "      3       203\n",
      "      4       188\n",
      "             ... \n",
      "9     9        28\n",
      "      10       20\n",
      "      11       14\n",
      "      12       11\n",
      "      13       36\n",
      "Length: 140, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Fold = StratifiedKFold(n_splits=config.n_fold, shuffle=True, random_state=seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[\"bins\"])):\n",
    "    train.loc[val_index, \"fold\"] = int(n)\n",
    "train[\"fold\"] = train[\"fold\"].astype(np.int8)\n",
    "print(train.groupby([\"fold\", \"bins\"]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcKN3QPeMMFf"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"crop image\" in wandb_tags:\n",
    "    TRAIN_IMAGE_PATH = DATA_DIR + \"crop/\"\n",
    "    print(\"Use croped images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1633394797331,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "pX4piTk2MPVR"
   },
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, label=True):\n",
    "        self.df = df\n",
    "        self.file_names = df[\"Id\"].values\n",
    "        self.features = df.drop([\"Id\", \"Pawpularity\", \"fold\", \"bins\"], axis=1).values\n",
    "        self.transform = transform\n",
    "\n",
    "        self.use_label = label\n",
    "        if self.use_label:\n",
    "            self.path = TRAIN_IMAGE_PATH\n",
    "            self.labels = df[\"Pawpularity\"].values / 100.0\n",
    "        else:\n",
    "            self.path = TEST_IMAGE_PATH\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f\"{self.path}/{file_name}.jpg\"\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        feature = torch.tensor(self.features[idx])\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "        if self.use_label:\n",
    "            label = torch.tensor(self.labels[idx])\n",
    "            return image, feature, label\n",
    "        return image, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1633394798564,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "0CIysnf0OhGK",
    "outputId": "dca880e9-0d92-43e7-82e2-a11ecacb54c1"
   },
   "outputs": [],
   "source": [
    "if Config.debug:\n",
    "    train_ds = BaseDataset(train)\n",
    "    image, feature, label = train_ds[0]\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"label: {label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionDataset(Dataset):\n",
    "    def __init__(self, df, force_input_size=None):\n",
    "        self.df = df\n",
    "        self.files = (f\"{TRAIN_IMAGE_PATH}/\" + df[\"Id\"] + \".jpg\").values\n",
    "\n",
    "        input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "        self.input_size = input_sizes[config.compound_coef] if force_input_size is None else force_input_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ori_img, framed_img, framed_meta = preprocess(self.files[idx], max_size=self.input_size)\n",
    "\n",
    "        x = torch.from_numpy(framed_img[0]).permute(2, 0, 1)\n",
    "\n",
    "        return x, ori_img, framed_img, framed_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.debug and Config.preprocess:\n",
    "    train_ds = DetectionDataset(train)\n",
    "    x, ori_img, framed_img, framed_meta = train_ds[0]\n",
    "    plt.imshow(framed_img[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633394797328,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "yJhnZyCOKf_8"
   },
   "outputs": [],
   "source": [
    "def get_transforms(*, data):\n",
    "    if data == \"train\":\n",
    "        if \"basic aug\" in wandb_tags:\n",
    "            return A.Compose(\n",
    "                [\n",
    "                    A.Resize(config.size, config.size),\n",
    "                    # A.RandomResizedCrop(config.size, config.size),\n",
    "                    # A.Transpose(p=0.5),\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    # A.VerticalFlip(p=0.5),\n",
    "                    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.5),\n",
    "                    A.Cutout(max_h_size=int(config.size * 0.4), max_w_size=int(config.size * 0.4), num_holes=1, p=0.5),\n",
    "                    A.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225],\n",
    "                    ),\n",
    "                    ToTensorV2(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        if \"heavy aug\" in wandb_tags:\n",
    "            return A.Compose(\n",
    "                [\n",
    "                    # A.Resize(config.size, config.size),\n",
    "                    A.RandomResizedCrop(config.size, config.size),\n",
    "                    A.Transpose(p=0.5),\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.VerticalFlip(p=0.5),\n",
    "                    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.5),\n",
    "                    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "                    # A.CoarseDropout(p=0.5),\n",
    "                    # A.Cutout(max_h_size=int(config.size * 0.4), max_w_size=int(config.size * 0.4), num_holes=1, p=0.5),\n",
    "                    A.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225],\n",
    "                    ),\n",
    "                    ToTensorV2(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(config.size, config.size),\n",
    "            # A.CenterCrop(config.size, config.size),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1633394798565,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "JwLeTm_EP9kK",
    "outputId": "9193e65d-197d-4e17-91d7-ee026dc0d38a"
   },
   "outputs": [],
   "source": [
    "if Config.debug:\n",
    "    train_ds = BaseDataset(train, transform=get_transforms(data=\"train\"))\n",
    "    image, feature, label = train_ds[0]\n",
    "    plt.imshow(image[0])\n",
    "    plt.title(f\"label: {label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1.0 - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "def cutmix(x, feats, y, alpha=1.0, use_cuda=True):\n",
    "    lam = np.clip(np.random.beta(alpha, alpha), 0.3, 0.4)\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    mixed_x = x.clone()\n",
    "    mixed_x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
    "    mixed_feats = lam * feats + (1 - lam) * feats[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "\n",
    "    return mixed_x, mixed_feats, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.debug and config.model_name != \"\":\n",
    "    train_ds = BaseDataset(train, transform=get_transforms(data=\"valid\"))\n",
    "    train_loader = DataLoader(train_ds, batch_size=12, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "    for image, feature, label in train_loader:\n",
    "        image, feature, label_a, label_b, lam = cutmix(image, feature, label, alpha=0.5, use_cuda=False)\n",
    "\n",
    "        plt.imshow(image[0].permute(1, 2, 0))\n",
    "        plt.title(f\"label: {label_a[0]} and {label_b[0]}\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"feature: {feature[0]}, lam: {lam}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1633394797331,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "BqcwfvhMterO"
   },
   "outputs": [],
   "source": [
    "# https://github.com/yuhao318/mwh/blob/main/utils.py\n",
    "def mixup(x, feats, y, alpha=1.0, use_cuda=True):\n",
    "\n",
    "    \"\"\"Compute the mixup data. Return mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "    if alpha > 0.0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        lam = max(lam, 1 - lam)\n",
    "        # lam = min(lam, 1-lam)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    ## NO SYM\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    mixed_feats = lam * feats + (1 - lam) * feats[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "\n",
    "    # return mixed_image, mixed_label, lam\n",
    "    return mixed_x, mixed_feats, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "CiCqVeWhh2gr",
    "outputId": "81496b08-c725-48b9-ad02-573eb0346269"
   },
   "outputs": [],
   "source": [
    "if Config.debug and config.model_name != \"\":\n",
    "    train_ds = BaseDataset(train, transform=get_transforms(data=\"valid\"))\n",
    "    train_loader = DataLoader(train_ds, batch_size=12, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "    for image, feature, label in train_loader:\n",
    "        image, feature, label_a, label_b, lam = mixup(image, feature, label, alpha=0.5, use_cuda=False)\n",
    "\n",
    "        plt.imshow(image[0].permute(1, 2, 0))\n",
    "        plt.title(f\"label: {label_a[0]} and {label_b[0]}\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"feature: {feature[0]}, lam: {lam}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbrC2J4KPEp4"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tmp-iclr/convmixer\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "\n",
    "def ConvMixer(dim, depth, kernel_size=9, patch_size=7, n_classes=1000):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[\n",
    "            nn.Sequential(\n",
    "                Residual(\n",
    "                    nn.Sequential(\n",
    "                        nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"), nn.GELU(), nn.BatchNorm2d(dim)\n",
    "                    )\n",
    "                ),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim),\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ],\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )\n",
    "\n",
    "\n",
    "_cfg = {\n",
    "    \"url\": \"\",\n",
    "    \"num_classes\": 1000,\n",
    "    \"input_size\": (3, 224, 224),\n",
    "    \"pool_size\": None,\n",
    "    \"crop_pct\": 0.96,\n",
    "    \"interpolation\": \"bicubic\",\n",
    "    \"mean\": timm.data.IMAGENET_DEFAULT_MEAN,\n",
    "    \"std\": timm.data.IMAGENET_DEFAULT_STD,\n",
    "    \"classifier\": \"head\",\n",
    "}\n",
    "\n",
    "\n",
    "@timm.models.registry.register_model\n",
    "def convmixer_1536_20(pretrained=False, **kwargs):\n",
    "    model = ConvMixer(1536, 20, kernel_size=9, patch_size=7, n_classes=1000)\n",
    "    model.default_cfg = _cfg\n",
    "    if pretrained:\n",
    "        path = \"../input/pretrained-weights/convmixer_1536_20_ks9_p7.pth.zip\"\n",
    "        timm.models.load_checkpoint(model, path, False)\n",
    "    return model\n",
    "\n",
    "\n",
    "@timm.models.registry.register_model\n",
    "def convmixer_768_32(pretrained=False, **kwargs):\n",
    "    model = ConvMixer(768, 32, kernel_size=7, patch_size=7, n_classes=1000)\n",
    "    model.default_cfg = _cfg\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1633394798566,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "827lYLgIPFyY"
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=config.n_class)\n",
    "\n",
    "        if \"convmixer\" in config.model_name:\n",
    "            self.head = nn.Linear(1000, config.n_class)\n",
    "\n",
    "    @amp.autocast(enabled=Config.amp)\n",
    "    def forward(self, x, feats):\n",
    "        x = self.model(x)\n",
    "        if \"convmixer\" in config.model_name:\n",
    "            x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode(model):\n",
    "    model.train()\n",
    "\n",
    "    # Freeze batch norm for efficientnet\n",
    "    if \"efficientnet\" in config.model_name:\n",
    "        for name1, child1 in model.named_children():\n",
    "            for name2, child2 in child1.named_children():\n",
    "                # print(f\"===== {name2} =====\")\n",
    "                if isinstance(child2, nn.BatchNorm2d):\n",
    "                    for param in child2.parameters():\n",
    "                        param.requires_grad = False\n",
    "                        # print(param.requires_grad)\n",
    "\n",
    "                for child3 in child2.children():\n",
    "                    if isinstance(child3, nn.modules.container.Sequential):\n",
    "                        for child4 in child3.children():\n",
    "                            for child5 in child4.children():\n",
    "                                if isinstance(child5, nn.BatchNorm2d):\n",
    "                                    # print(child5)\n",
    "                                    for param in child5.parameters():\n",
    "                                        param.requires_grad = False\n",
    "                                        # print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83590,
     "status": "ok",
     "timestamp": 1633394882149,
     "user": {
      "displayName": "Yoshio S",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02953556559298393836"
     },
     "user_tz": -540
    },
    "id": "oudAIpPqQt6z",
    "outputId": "32564315-dd7d-4bc9-f24b-620488993b3a"
   },
   "outputs": [],
   "source": [
    "if Config.debug and config.model_name != \"\" and config.model_embedder == \"\":\n",
    "    model = BaseModel(config.model_name)\n",
    "    print(model)\n",
    "\n",
    "    train_ds = BaseDataset(train, transform=get_transforms(data=\"train\"))\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "    for image, feature, label in train_loader:\n",
    "        output = model(image, feature)\n",
    "        print(output)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.debug and config.model_name != \"\" and config.model_embedder == \"\":\n",
    "    model = BaseModel(config.model_name)\n",
    "    model.apply(train_mode)\n",
    "\n",
    "    for n, (name, tensor) in enumerate(list(model.named_parameters())):\n",
    "        print(f\"{n:>4}: {tensor.requires_grad}, {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneModel(nn.Module):\n",
    "    def __init__(self, model_name, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "    @amp.autocast(enabled=Config.amp)\n",
    "    def forward(self, x, feats):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat([x, feats], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.train2 and Config.debug and config.model_name != \"\":\n",
    "    model = BackboneModel(config.model_name)\n",
    "    print(model)\n",
    "\n",
    "    train_ds = BaseDataset(train, transform=get_transforms(data=\"train\"))\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "    for image, feature, label in train_loader:\n",
    "        output = model(image, feature)\n",
    "        print(output)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid swin transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEmbed(nn.Module):\n",
    "    \"\"\"CNN Feature Map Embedding\n",
    "    Extract feature map from CNN, flatten, project to embedding dim.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, img_size=224, patch_size=1, feature_size=None, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        assert isinstance(backbone, nn.Module)\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.backbone = backbone\n",
    "        if feature_size is None:\n",
    "            with torch.no_grad():\n",
    "                # NOTE Most reliable way of determining output dims is to run forward pass\n",
    "                training = backbone.training\n",
    "                if training:\n",
    "                    backbone.eval()\n",
    "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))\n",
    "                if isinstance(o, (list, tuple)):\n",
    "                    o = o[-1]  # last feature if backbone outputs list/tuple of features\n",
    "                feature_size = o.shape[-2:]\n",
    "                feature_dim = o.shape[1]\n",
    "                backbone.train(training)\n",
    "        else:\n",
    "            feature_size = to_2tuple(feature_size)\n",
    "            if hasattr(self.backbone, \"feature_info\"):\n",
    "                feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "            else:\n",
    "                feature_dim = self.backbone.num_features\n",
    "        assert feature_size[0] % patch_size[0] == 0 and feature_size[1] % patch_size[1] == 0\n",
    "        self.grid_size = (feature_size[0] // patch_size[0], feature_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.proj = nn.Conv2d(feature_dim, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            x = x[-1]  # last feature if backbone outputs list/tuple of features\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSwinModel(nn.Module):\n",
    "    def __init__(self, backbone, embedder, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.embedder = timm.create_model(embedder, features_only=True, out_indices=[2], pretrained=pretrained)\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained, num_classes=config.n_class)\n",
    "        self.backbone.patch_embed = HybridEmbed(self.embedder, img_size=config.size, embed_dim=128)\n",
    "\n",
    "    @amp.autocast(enabled=Config.amp)\n",
    "    def forward(self, images, _):\n",
    "        output = self.backbone(images)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.debug and config.model_name != \"\" and config.model_embedder != \"\":\n",
    "    model = HybridSwinModel(config.model_name, config.model_embedder)\n",
    "    print(model)\n",
    "\n",
    "    train_ds = BaseDataset(train, transform=get_transforms(data=\"train\"))\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "    for image, feature, label in train_loader:\n",
    "        output = model(image, feature)\n",
    "        print(output)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCa8k53cRIi_"
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "FbkDI8DdRJ5w"
   },
   "outputs": [],
   "source": [
    "# https://github.com/davda54/sam/blob/main/sam.py\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    @torch.inference_mode()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad:\n",
    "            self.zero_grad(set_to_none=True)\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    @torch.inference_mode()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad:\n",
    "            self.zero_grad(set_to_none=True)\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    @torch.inference_mode()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][\n",
    "            0\n",
    "        ].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "            torch.stack(\n",
    "                [\n",
    "                    ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                    for group in self.param_groups\n",
    "                    for p in group[\"params\"]\n",
    "                    if p.grad is not None\n",
    "                ]\n",
    "            ),\n",
    "            p=2,\n",
    "        )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFMHkJIMRLBi"
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "f6jKOTHURLxM"
   },
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWbNT_NeRMoB"
   },
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "S4EF_M0NROIn"
   },
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0fQUwt7adOSk"
   },
   "outputs": [],
   "source": [
    "def get_result(result_df, fold=config.n_fold):\n",
    "    preds = result_df[\"preds\"].values\n",
    "    labels = result_df[\"Pawpularity\"].values\n",
    "    score = get_score(labels, preds)\n",
    "    LOGGER.info(f\"Score: {score:<.5f}\")\n",
    "    if fold == config.n_fold:\n",
    "        wandb.log({\"Score\": score})\n",
    "    else:\n",
    "        wandb.log({f\"Score_fold{fold}\": score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8Ga4Bb8RQsF"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "rRPW8QwiRbPC"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "23roIn-jhfMe"
   },
   "outputs": [],
   "source": [
    "# https://github.com/Bjarten/early-stopping-pytorch\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path=\"checkpoint.pt\", trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        self.best_preds = None\n",
    "\n",
    "    def __call__(self, val_loss, score, model, preds):\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_preds = preds\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss >= self.val_loss_min + self.delta:\n",
    "            if self.patience <= 0:\n",
    "                return\n",
    "            self.counter += 1\n",
    "            self.trace_func(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_preds = preds\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            self.trace_func(\n",
    "                f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\"\n",
    "            )\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "l9Kcw2bdeZR4"
   },
   "outputs": [],
   "source": [
    "def compute_grad_norm(parameters, norm_type=2.0):\n",
    "    \"\"\"Refer to torch.nn.utils.clip_grad_norm_\"\"\"\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    total_norm = 0\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm.item() ** norm_type\n",
    "    total_norm = total_norm ** (1.0 / norm_type)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(data_loader, model, device):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "sWtO4py7Rcud",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, scheduler, scaler, epoch, device):\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    if \"freeze norm\" in wandb_tags:\n",
    "        model.apply(train_mode)\n",
    "    else:\n",
    "        model.train()\n",
    "    start = time.time()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (images, features, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        if epoch < 2 or epoch >= config.epochs - 2:\n",
    "            mix_decision = 1.0\n",
    "        else:\n",
    "            mix_decision = np.random.rand()\n",
    "\n",
    "        if epoch >= config.epochs - 5:\n",
    "            mix_decision *= 2\n",
    "\n",
    "        if \"mixup\" in wandb_tags and mix_decision < 0.5:\n",
    "            images, features, label_a, label_b, lam = mixup(images, features, labels, alpha=0.5)\n",
    "\n",
    "        with amp.autocast(enabled=Config.amp):\n",
    "            # y_preds = model(images, features)\n",
    "            y_preds = model(images, features).squeeze(1)\n",
    "\n",
    "            if \"mixup\" in wandb_tags and mix_decision < 0.5:\n",
    "                loss = criterion(y_preds, label_a) * lam + criterion(y_preds, label_b) * (1.0 - lam)\n",
    "            else:\n",
    "                loss = criterion(y_preds, labels)\n",
    "\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            loss = loss / config.gradient_accumulation_steps\n",
    "\n",
    "            if \"SAM\" in config.optimizer:\n",
    "\n",
    "                def closure():\n",
    "                    # y_preds = model(images, features)\n",
    "                    y_preds = model(images, features).squeeze(1)\n",
    "\n",
    "                    if \"mixup\" in wandb_tags and mix_decision < 0.5:\n",
    "                        loss = criterion(y_preds, label_a) * lam + criterion(y_preds, label_b) * (1.0 - lam)\n",
    "                    else:\n",
    "                        loss = criterion(y_preds, labels)\n",
    "\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "\n",
    "            if \"SAM\" in config.optimizer:\n",
    "                scaler.step(optimizer, closure)\n",
    "            else:\n",
    "                scaler.step(optimizer)\n",
    "\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            grad_norm = compute_grad_norm(model.parameters())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % Config.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print(\n",
    "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "                f\"Grad: {grad_norm:.4f} \"\n",
    "                f\"LR: {scheduler.get_lr()[0]:.2e}  \"\n",
    "            )\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "wlPZYJYgXRZ4"
   },
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (images, features, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        with torch.inference_mode():\n",
    "            # y_preds = model(images, features)\n",
    "            y_preds = model(images, features).squeeze(1)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # preds.append(y_preds.softmax(1).to(\"cpu\").numpy())\n",
    "        preds.append(y_preds.to(\"cpu\").numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % Config.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print(\n",
    "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training2 (Inference by backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2_inference_fn(data_loader, model, device):\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (images, features, labels) in enumerate(data_loader):\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        with torch.inference_mode():\n",
    "            y_preds = model(images, features)\n",
    "            # y_preds = model(images, features).squeeze(1)\n",
    "\n",
    "        # preds.append(y_preds.softmax(1).to(\"cpu\").numpy())\n",
    "        preds.append(y_preds.to(\"cpu\").numpy())\n",
    "\n",
    "        end = time.time()\n",
    "        if step % Config.print_freq == 0 or step == (len(data_loader) - 1):\n",
    "            print(\n",
    "                f\"EVAL: [{step}/{len(data_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(data_loader)):s} \"\n",
    "            )\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ybZNlV6YEp8"
   },
   "source": [
    "## Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JIDycP9YFvO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(preds, imgs, files, imshow=False, imwrite=True):\n",
    "    for i in range(len(imgs)):\n",
    "        # if len(preds[i]['rois']) == 0:\n",
    "        #     continue\n",
    "\n",
    "        imgs[i] = imgs[i].copy()\n",
    "\n",
    "        for j in range(len(preds[i][\"rois\"])):\n",
    "            # cat or dog\n",
    "            if preds[i][\"class_ids\"][j] not in (16, 17):\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = preds[i][\"rois\"][j].astype(np.int)\n",
    "            # obj = obj_list[preds[i][\"class_ids\"][j]]\n",
    "            # score = float(preds[i][\"scores\"][j])\n",
    "            # plot_one_box(\n",
    "            #     imgs[i], [x1, y1, x2, y2], label=obj, score=score, color=color_list[get_index_label(obj, obj_list)]\n",
    "            # )\n",
    "\n",
    "            # Crop image\n",
    "            imgs[i] = imgs[i][y1:y2, x1:x2, :]\n",
    "\n",
    "            # Apply only highest preds\n",
    "            break\n",
    "\n",
    "        if imshow:\n",
    "            cv2.imshow(\"img\", imgs[i])\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "        if imwrite:\n",
    "            cv2.imwrite(f\"{DATA_DIR}/crop/{os.path.basename(files[i])}\", imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_loop(df):\n",
    "    crop_output_dir = f\"{DATA_DIR}/crop/\"\n",
    "    # !rm -rf {crop_output_dir}\n",
    "    os.makedirs(crop_output_dir, exist_ok=True)\n",
    "\n",
    "    # replace this part with your project's anchor config\n",
    "    anchor_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\n",
    "    anchor_scales = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\n",
    "\n",
    "    threshold = 0.2\n",
    "    iou_threshold = 0.2\n",
    "\n",
    "    color_list = standard_to_bgr(STANDARD_COLORS)\n",
    "    # tf bilinear interpolation is different from any other's, just make do\n",
    "    force_input_size = None  # set None to use default size\n",
    "    input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "    input_size = input_sizes[config.compound_coef] if force_input_size is None else force_input_size\n",
    "\n",
    "    model = EfficientDetBackbone(\n",
    "        compound_coef=config.compound_coef, num_classes=90, ratios=anchor_ratios, scales=anchor_scales\n",
    "    )\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"../input/pretrained-weights/efficientdet-d{config.compound_coef}.pth\", map_location=\"cpu\")\n",
    "    )\n",
    "\n",
    "    if Config.multi_gpu:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "\n",
    "    files = (f\"{TRAIN_IMAGE_PATH}/\" + df[\"Id\"] + \".jpg\").values\n",
    "\n",
    "    for file_ in tqdm(files, total=len(files)):\n",
    "        ori_imgs, framed_imgs, framed_metas = preprocess(file_, max_size=input_size)\n",
    "        x = torch.stack([torch.from_numpy(fi).to(device) for fi in framed_imgs], 0).permute(0, 3, 1, 2)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        with torch.inference_mode():\n",
    "            _, regression, classification, anchors = model(x)\n",
    "\n",
    "            regressBoxes = BBoxTransform()\n",
    "            clipBoxes = ClipBoxes()\n",
    "\n",
    "            out = postprocess(x, anchors, regression, classification, regressBoxes, clipBoxes, threshold, iou_threshold)\n",
    "\n",
    "        out = invert_affine(framed_metas, out)\n",
    "        save_image(out, ori_imgs, [file_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mwWqWJdX232"
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "ei3alnONX4RY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(df, fold):\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Data Loader\n",
    "    # ====================================================\n",
    "    trn_idx = df[df[\"fold\"] != fold].index\n",
    "    val_idx = df[df[\"fold\"] == fold].index\n",
    "\n",
    "    train_folds = df.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = BaseDataset(train_folds, transform=get_transforms(data=\"train\"))\n",
    "    train_dataset_ = BaseDataset(train_folds, transform=get_transforms(data=\"valid\"))\n",
    "    valid_dataset = BaseDataset(valid_folds, transform=get_transforms(data=\"valid\"))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    train_loader_ = DataLoader(\n",
    "        train_dataset_,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Optimizer\n",
    "    # ====================================================\n",
    "    def get_optimizer(model):\n",
    "        if config.optimizer == \"Adam\":\n",
    "            optimizer = Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "        elif config.optimizer == \"AdamW\":\n",
    "            optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "        elif config.optimizer == \"SAM\":\n",
    "            base_optimizer = SGD\n",
    "            optimizer = SAM(\n",
    "                model.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=config.lr,\n",
    "                momentum=config.momentum,\n",
    "                weight_decay=config.weight_decay,\n",
    "            )\n",
    "        elif config.optimizer == \"ASAM\":\n",
    "            base_optimizer = SGD\n",
    "            optimizer = SAM(\n",
    "                model.parameters(),\n",
    "                base_optimizer,\n",
    "                lr=config.lr,\n",
    "                momentum=config.momentum,\n",
    "                weight_decay=config.weight_decay,\n",
    "                rho=2.0,\n",
    "                adaptive=True,\n",
    "            )\n",
    "\n",
    "        if \"agc\" in wandb_tags and \"nfnet\" in config.model_name:\n",
    "            if Config.multi_gpu:\n",
    "                ignore_agc = [\"module.model.head.fc\"]\n",
    "            else:\n",
    "                ignore_agc = [\"model.head.fc\"]\n",
    "\n",
    "            optimizer = AGC(\n",
    "                model.parameters(),\n",
    "                optimizer,\n",
    "                clipping=config.agc_clipping,\n",
    "                model=model,\n",
    "                ignore_agc=ignore_agc,\n",
    "            )\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    # ====================================================\n",
    "    # Scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer, train_dataset):\n",
    "        num_data = len(train_dataset)\n",
    "        num_steps = num_data // (config.batch_size * config.gradient_accumulation_steps) * config.epochs\n",
    "\n",
    "        if config.scheduler == \"CosineAnnealingWarmRestarts\":\n",
    "            scheduler = CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=num_steps, T_mult=1, eta_min=config.min_lr, last_epoch=-1\n",
    "            )\n",
    "        elif config.scheduler == \"CosineAnnealingLR\":\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=num_steps, eta_min=config.min_lr, last_epoch=-1)\n",
    "        elif config.scheduler == \"CosineAnnealingWarmupRestarts\":\n",
    "            scheduler = CosineAnnealingWarmupRestarts(\n",
    "                optimizer,\n",
    "                first_cycle_steps=num_steps,\n",
    "                max_lr=config.lr,\n",
    "                min_lr=config.min_lr,\n",
    "                warmup_steps=(num_steps // 10),\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # Model\n",
    "    # ====================================================\n",
    "    if config.model_embedder == \"\":\n",
    "        model = BaseModel(config.model_name)\n",
    "    else:\n",
    "        model = HybridSwinModel(config.model_name, config.model_embedder)\n",
    "\n",
    "    if Config.multi_gpu:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = get_optimizer(model)\n",
    "    scaler = amp.GradScaler(enabled=Config.amp)\n",
    "    scheduler = get_scheduler(optimizer, train_dataset)\n",
    "\n",
    "    # ====================================================\n",
    "    # Criterion\n",
    "    # ====================================================\n",
    "    def get_criterion():\n",
    "        if config.criterion == \"CrossEntropyLoss\":\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif config.criterion == \"BCEWithLogitsLoss\":\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "        elif config.criterion == \"MSELoss\":\n",
    "            criterion = nn.MSELoss()\n",
    "        return criterion\n",
    "\n",
    "    criterion = get_criterion()\n",
    "\n",
    "    # ====================================================\n",
    "    # Early stopping\n",
    "    # ====================================================\n",
    "    es = EarlyStopping(\n",
    "        patience=config.es_patience,\n",
    "        verbose=True,\n",
    "        path=MODEL_DIR + f\"{config.model_name.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Loop\n",
    "    # ====================================================\n",
    "    best_score = -1\n",
    "    best_loss = np.inf\n",
    "    best_preds = None\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        if epoch < 2 or epoch >= config.epochs - 2:\n",
    "            avg_loss = train_fn(train_loader_, model, criterion, optimizer, scheduler, scaler, epoch, device)\n",
    "        else:\n",
    "            avg_loss = train_fn(train_loader, model, criterion, optimizer, scheduler, scaler, epoch, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        valid_labels = valid_folds[\"Pawpularity\"].values\n",
    "\n",
    "        if config.criterion == \"BCEWithLogitsLoss\":\n",
    "            preds = 1 / (1 + np.exp(-preds))\n",
    "\n",
    "        preds *= 100.0\n",
    "\n",
    "        # scoring\n",
    "        # score = get_score(valid_labels, preds.argmax(1))\n",
    "        score = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
    "        )\n",
    "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score}\")\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                f\"loss/train_fold{fold}\": avg_loss,\n",
    "                f\"loss/valid_fold{fold}\": avg_val_loss,\n",
    "                f\"score/fold{fold}\": score,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        es(avg_val_loss, score, model, preds)\n",
    "\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # valid_folds[[str(c) for c in range(config.n_class)]] = es.best_preds\n",
    "    # valid_folds[\"preds\"] = es.best_preds.argmax(1)\n",
    "    valid_folds[\"preds\"] = es.best_preds\n",
    "\n",
    "    return valid_folds, es.best_score, es.val_loss_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train2 loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2_loop(df, artifact_id, fold):\n",
    "    LOGGER.info(f\"========== ID: {artifact_id} fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Data Loader\n",
    "    # ====================================================\n",
    "    trn_idx = df[df[\"fold\"] != fold].index\n",
    "    val_idx = df[df[\"fold\"] == fold].index\n",
    "\n",
    "    train_folds = df.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = df.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = BaseDataset(train_folds, transform=get_transforms(data=\"valid\"))\n",
    "    valid_dataset = BaseDataset(valid_folds, transform=get_transforms(data=\"valid\"))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    valid_labels = valid_folds[\"Pawpularity\"].values\n",
    "\n",
    "    # ====================================================\n",
    "    # Model\n",
    "    # ====================================================\n",
    "    model_name = artifact_id.split(\":\")[0]\n",
    "    model = BackboneModel(model_name)\n",
    "    if Config.multi_gpu:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    path = f\"{artifact_id.replace(':', '-')}/{model_name}_fold{fold}_best.pth\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    # ====================================================\n",
    "    # Inference by backbone\n",
    "    # ====================================================\n",
    "\n",
    "    train_preds = train2_inference_fn(train_loader, model, device)\n",
    "    valid_preds = train2_inference_fn(valid_loader, model, device)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    # scaler.fit(np.vstack([train_preds, valid_preds]))\n",
    "\n",
    "    train_norm = scaler.fit_transform(train_preds)\n",
    "    valid_norm = scaler.transform(valid_preds)\n",
    "\n",
    "    scaler_path = MODEL_DIR + f\"{model_name}-StandardScaler_fold{fold}_best.pkl\"\n",
    "    pickle.dump(scaler, open(scaler_path, \"wb\"))\n",
    "\n",
    "    # ====================================================\n",
    "    # Tuning SVR parameters\n",
    "    # ====================================================\n",
    "    # https://github.com/hkaneko1985/fastoptsvrhyperparams/blob/master/fastoptsvrhyperparams.ipynb\n",
    "    start_time = time.time()\n",
    "\n",
    "    svr_epss = 2 ** np.arange(-10, 1, dtype=float)  # Candidates of epsilon\n",
    "    svr_cs = 2 ** np.arange(-5, 11, dtype=float)  # Candidates of C\n",
    "    svr_gammas = 2 ** np.arange(-20, 11, dtype=float)  # Candidates of gamma\n",
    "\n",
    "    # Optimize epsilon with cross-validation\n",
    "    model_tune_eps = GridSearchCV(SVR(kernel=\"rbf\", C=3), {\"epsilon\": svr_epss}, verbose=1)\n",
    "    model_tune_eps.fit(train_norm, train_dataset.labels)\n",
    "    optimal_eps = model_tune_eps.best_params_[\"epsilon\"]\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Optimized eps: {elapsed:.0f}s\")\n",
    "\n",
    "    # Optimize C with cross-validation\n",
    "    model_tune_c = GridSearchCV(SVR(kernel=\"rbf\", epsilon=optimal_eps), {\"C\": svr_cs}, verbose=1)\n",
    "    model_tune_c.fit(train_norm, train_dataset.labels)\n",
    "    optimal_c = model_tune_c.best_params_[\"C\"]\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Optimized c: {elapsed:.0f}s\")\n",
    "\n",
    "    # Optimize gamma with cross-validation (optional)\n",
    "    model_tune_gamma = GridSearchCV(\n",
    "        SVR(kernel=\"rbf\", epsilon=optimal_eps, C=optimal_c), {\"gamma\": svr_gammas}, verbose=1\n",
    "    )\n",
    "    model_tune_gamma.fit(train_norm, train_dataset.labels)\n",
    "    optimal_gamma = model_tune_gamma.best_params_[\"gamma\"]\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Optimized gamma: {elapsed:.0f}s\")\n",
    "\n",
    "    best_params = {\n",
    "        \"C\": optimal_c,\n",
    "        \"epsilon\": optimal_eps,\n",
    "        \"gamma\": optimal_gamma,\n",
    "    }\n",
    "    print(f\"{best_params}\")\n",
    "\n",
    "    param_path = MODEL_DIR + f\"{model_name}-SVR-params_fold{fold}_best.json\"\n",
    "    with open(param_path, \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    # ====================================================\n",
    "    # Training SVR\n",
    "    # ====================================================\n",
    "    start_time = time.time()\n",
    "\n",
    "    model_svr = SVR(C=optimal_c, epsilon=optimal_eps, gamma=optimal_gamma)\n",
    "    model_svr.fit(train_norm, train_dataset.labels)\n",
    "\n",
    "    preds = model_svr.predict(valid_norm)\n",
    "    preds *= 100.0\n",
    "\n",
    "    # scoring\n",
    "    # score = get_score(valid_labels, preds.argmax(1))\n",
    "    score = get_score(valid_labels, preds)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    LOGGER.info(f\"Score: {score}  time: {elapsed:.0f}s\")\n",
    "\n",
    "    svr_path = MODEL_DIR + f\"{model_name}-SVR_fold{fold}_best.pkl\"\n",
    "    pickle.dump(model_svr, open(svr_path, \"wb\"))\n",
    "\n",
    "    # valid_folds[[str(c) for c in range(config.n_class)]] = es.best_preds\n",
    "    # valid_folds[\"preds\"] = es.best_preds.argmax(1)\n",
    "    # valid_folds[\"preds\"] = es.best_preds\n",
    "    valid_folds[\"preds\"] = preds\n",
    "\n",
    "    return valid_folds, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVj6cfuLc4VP"
   },
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "PpdJfNgUc5N3"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ====================================================\n",
    "    # Preprocess\n",
    "    # ====================================================\n",
    "    if Config.preprocess:\n",
    "        preprocess_loop(train)\n",
    "\n",
    "    # ====================================================\n",
    "    # Training\n",
    "    # ====================================================\n",
    "    if Config.train or Config.train3:\n",
    "        oof_df = pd.DataFrame()\n",
    "        oof_result = []\n",
    "        for fold in range(config.n_fold):\n",
    "            seed_torch(seed + fold)\n",
    "\n",
    "            _oof_df, score, loss = train_loop(train, fold)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            oof_result.append([fold, score, loss])\n",
    "\n",
    "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "            get_result(_oof_df, fold)\n",
    "\n",
    "            if \"tune lr\" in wandb_tags:\n",
    "                break\n",
    "\n",
    "        # CV result\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "\n",
    "        loss = statistics.mean([d[2] for d in oof_result])\n",
    "        wandb.log({\"loss\": loss})\n",
    "\n",
    "        table = wandb.Table(data=oof_result, columns=[\"fold\", \"score\", \"loss\"])\n",
    "        run.log({\"Fold Result\": table})\n",
    "\n",
    "        # save result\n",
    "        oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
    "        wandb.save(OUTPUT_DIR + \"oof_df.csv\")\n",
    "\n",
    "        artifact = wandb.Artifact(config.model_name, type=\"model\")\n",
    "        artifact.add_dir(MODEL_DIR)\n",
    "        run.log_artifact(artifact)\n",
    "\n",
    "    # ====================================================\n",
    "    # Validation\n",
    "    # ====================================================\n",
    "    if Config.validate:\n",
    "        cols = [f\"preds{n}\" for n in range(len(config.runs))]\n",
    "        train[\"preds\"] = train[cols].values.mean(axis=1)\n",
    "\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(train)\n",
    "\n",
    "        # save result\n",
    "        train.to_csv(OUTPUT_DIR + \"validation_df.csv\", index=False)\n",
    "        wandb.save(OUTPUT_DIR + \"validation_df.csv\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Training SVR\n",
    "    # ====================================================\n",
    "    if Config.train2:\n",
    "        oof_df = pd.DataFrame()\n",
    "        oof_result = []\n",
    "        for n, artifact_id in enumerate(config.models):\n",
    "\n",
    "            for fold in range(config.n_fold):\n",
    "                seed_torch(seed + fold)\n",
    "                _oof_df, score = train2_loop(train, artifact_id, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df, fold)\n",
    "\n",
    "            # CV result\n",
    "            LOGGER.info(f\"========== CV ==========\")\n",
    "            get_result(oof_df)\n",
    "\n",
    "            break\n",
    "\n",
    "        # save result\n",
    "        oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
    "        wandb.save(OUTPUT_DIR + \"oof_df.csv\")\n",
    "\n",
    "        artifact = wandb.Artifact(f\"{artifact_id.split(':')[0]}-SVR\", type=\"model\")\n",
    "        artifact.add_dir(MODEL_DIR)\n",
    "        run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-J0CDXjkUaS"
   },
   "source": [
    "# üöÄ Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeDBzpKHdIie"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/557] Elapsed 0m 18s (remain 175m 45s) Loss: 0.6870 Grad: 8177.9212 LR: 5.00e-06  \n",
      "Epoch: [1][100/557] Elapsed 1m 1s (remain 4m 35s) Loss: 0.6665 Grad: 18530.5879 LR: 5.00e-06  \n",
      "Epoch: [1][200/557] Elapsed 1m 43s (remain 3m 2s) Loss: 0.6605 Grad: 10964.2776 LR: 5.00e-06  \n",
      "Epoch: [1][300/557] Elapsed 2m 24s (remain 2m 3s) Loss: 0.6580 Grad: 27310.8594 LR: 5.00e-06  \n",
      "Epoch: [1][400/557] Elapsed 3m 6s (remain 1m 12s) Loss: 0.6565 Grad: 7894.9321 LR: 5.00e-06  \n",
      "Epoch: [1][500/557] Elapsed 3m 48s (remain 0m 25s) Loss: 0.6547 Grad: 16216.5100 LR: 5.00e-06  \n",
      "Epoch: [1][556/557] Elapsed 4m 11s (remain 0m 0s) Loss: 0.6538 Grad: 18460.4398 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 4s) Loss: 0.6594 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.6538  avg_val_loss: 0.6451  time: 265s\n",
      "Epoch 1 - Score: 18.101897448851595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6451 \n",
      "Validation loss decreased (inf --> 0.645088).  Saving model ...\n",
      "Epoch: [2][0/557] Elapsed 0m 1s (remain 11m 37s) Loss: 0.6351 Grad: 9675.8132 LR: 5.00e-06  \n",
      "Epoch: [2][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6405 Grad: 16423.1784 LR: 5.00e-06  \n",
      "Epoch: [2][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6397 Grad: 6562.3151 LR: 5.00e-06  \n",
      "Epoch: [2][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.6394 Grad: 24839.6127 LR: 5.00e-06  \n",
      "Epoch: [2][400/557] Elapsed 2m 49s (remain 1m 5s) Loss: 0.6381 Grad: 10361.1747 LR: 5.00e-06  \n",
      "Epoch: [2][500/557] Elapsed 3m 30s (remain 0m 23s) Loss: 0.6381 Grad: 32565.7628 LR: 5.00e-06  \n",
      "Epoch: [2][556/557] Elapsed 3m 53s (remain 0m 0s) Loss: 0.6381 Grad: 14954.2254 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 5s) Loss: 0.6536 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.6381  avg_val_loss: 0.6426  time: 247s\n",
      "Epoch 2 - Score: 17.77218696443636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6426 \n",
      "Validation loss decreased (0.645088 --> 0.642640).  Saving model ...\n",
      "Epoch: [3][0/557] Elapsed 0m 1s (remain 14m 12s) Loss: 0.6253 Grad: 25950.9462 LR: 5.00e-06  \n",
      "Epoch: [3][100/557] Elapsed 0m 43s (remain 3m 15s) Loss: 0.6369 Grad: 26076.4057 LR: 5.00e-06  \n",
      "Epoch: [3][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6347 Grad: 9609.2132 LR: 5.00e-06  \n",
      "Epoch: [3][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.6337 Grad: 39802.6951 LR: 5.00e-06  \n",
      "Epoch: [3][400/557] Elapsed 2m 49s (remain 1m 5s) Loss: 0.6332 Grad: 7698.0863 LR: 5.00e-06  \n",
      "Epoch: [3][500/557] Elapsed 3m 31s (remain 0m 23s) Loss: 0.6331 Grad: 20888.1229 LR: 5.00e-06  \n",
      "Epoch: [3][556/557] Elapsed 3m 54s (remain 0m 0s) Loss: 0.6331 Grad: 50723.7619 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 8s) Loss: 0.6495 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.6331  avg_val_loss: 0.6418  time: 248s\n",
      "Epoch 3 - Score: 17.635714815316064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6418 \n",
      "Validation loss decreased (0.642640 --> 0.641799).  Saving model ...\n",
      "Epoch: [4][0/557] Elapsed 0m 1s (remain 11m 40s) Loss: 0.6022 Grad: 5544.0854 LR: 5.00e-06  \n",
      "Epoch: [4][100/557] Elapsed 0m 42s (remain 3m 12s) Loss: 0.6285 Grad: 26884.7434 LR: 5.00e-06  \n",
      "Epoch: [4][200/557] Elapsed 1m 23s (remain 2m 28s) Loss: 0.6273 Grad: 10845.9938 LR: 5.00e-06  \n",
      "Epoch: [4][300/557] Elapsed 2m 4s (remain 1m 46s) Loss: 0.6277 Grad: 33176.7328 LR: 5.00e-06  \n",
      "Epoch: [4][400/557] Elapsed 2m 45s (remain 1m 4s) Loss: 0.6271 Grad: 22791.5619 LR: 5.00e-06  \n",
      "Epoch: [4][500/557] Elapsed 3m 26s (remain 0m 23s) Loss: 0.6265 Grad: 58692.4474 LR: 5.00e-06  \n",
      "Epoch: [4][556/557] Elapsed 3m 49s (remain 0m 0s) Loss: 0.6269 Grad: 38959.3260 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 12s) Loss: 0.6449 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.6269  avg_val_loss: 0.6442  time: 243s\n",
      "Epoch 4 - Score: 17.918583918118053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6442 \n",
      "Epoch: [5][0/557] Elapsed 0m 1s (remain 13m 40s) Loss: 0.6500 Grad: 13947.2866 LR: 5.00e-06  \n",
      "Epoch: [5][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6239 Grad: 37259.2899 LR: 5.00e-06  \n",
      "Epoch: [5][200/557] Elapsed 1m 26s (remain 2m 32s) Loss: 0.6222 Grad: 41193.3517 LR: 5.00e-06  \n",
      "Epoch: [5][300/557] Elapsed 2m 8s (remain 1m 49s) Loss: 0.6204 Grad: 41506.6631 LR: 5.00e-06  \n",
      "Epoch: [5][400/557] Elapsed 2m 50s (remain 1m 6s) Loss: 0.6199 Grad: 17049.1306 LR: 5.00e-06  \n",
      "Epoch: [5][500/557] Elapsed 3m 32s (remain 0m 23s) Loss: 0.6203 Grad: 31068.5385 LR: 5.00e-06  \n",
      "Epoch: [5][556/557] Elapsed 3m 55s (remain 0m 0s) Loss: 0.6202 Grad: 42413.6613 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 4s) Loss: 0.6415 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.6202  avg_val_loss: 0.6484  time: 249s\n",
      "Epoch 5 - Score: 18.398459726966426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6484 \n",
      "Epoch: [6][0/557] Elapsed 0m 1s (remain 13m 6s) Loss: 0.6448 Grad: 12743.6789 LR: 5.00e-06  \n",
      "Epoch: [6][100/557] Elapsed 0m 43s (remain 3m 17s) Loss: 0.6138 Grad: 39772.2312 LR: 5.00e-06  \n",
      "Epoch: [6][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6138 Grad: 34867.4805 LR: 5.00e-06  \n",
      "Epoch: [6][300/557] Elapsed 2m 8s (remain 1m 48s) Loss: 0.6119 Grad: 51887.2054 LR: 5.00e-06  \n",
      "Epoch: [6][400/557] Elapsed 2m 50s (remain 1m 6s) Loss: 0.6125 Grad: 24009.1506 LR: 5.00e-06  \n",
      "Epoch: [6][500/557] Elapsed 3m 31s (remain 0m 23s) Loss: 0.6126 Grad: 32801.3394 LR: 5.00e-06  \n",
      "Epoch: [6][556/557] Elapsed 3m 54s (remain 0m 0s) Loss: 0.6125 Grad: 52377.7195 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 0s (remain 0m 59s) Loss: 0.6343 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.6125  avg_val_loss: 0.6503  time: 248s\n",
      "Epoch 6 - Score: 18.624768736568612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6503 \n",
      "Epoch: [7][0/557] Elapsed 0m 1s (remain 11m 37s) Loss: 0.6176 Grad: 27030.3991 LR: 5.00e-06  \n",
      "Epoch: [7][100/557] Elapsed 0m 42s (remain 3m 12s) Loss: 0.6055 Grad: 37465.6194 LR: 5.00e-06  \n",
      "Epoch: [7][200/557] Elapsed 1m 23s (remain 2m 28s) Loss: 0.6063 Grad: 12163.5613 LR: 5.00e-06  \n",
      "Epoch: [7][300/557] Elapsed 2m 5s (remain 1m 46s) Loss: 0.6048 Grad: 75122.9446 LR: 5.00e-06  \n",
      "Epoch: [7][400/557] Elapsed 2m 46s (remain 1m 4s) Loss: 0.6049 Grad: 26624.0057 LR: 5.00e-06  \n",
      "Epoch: [7][500/557] Elapsed 3m 26s (remain 0m 23s) Loss: 0.6043 Grad: 30852.9053 LR: 5.00e-06  \n",
      "Epoch: [7][556/557] Elapsed 3m 49s (remain 0m 0s) Loss: 0.6047 Grad: 35043.1105 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 1s) Loss: 0.6406 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.6047  avg_val_loss: 0.6528  time: 243s\n",
      "Epoch 7 - Score: 18.855725634563434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6528 \n",
      "Epoch: [8][0/557] Elapsed 0m 1s (remain 12m 11s) Loss: 0.6376 Grad: 35137.4043 LR: 5.00e-06  \n",
      "Epoch: [8][100/557] Elapsed 0m 43s (remain 3m 15s) Loss: 0.5999 Grad: 53232.3986 LR: 5.00e-06  \n",
      "Epoch: [8][200/557] Elapsed 1m 25s (remain 2m 30s) Loss: 0.5989 Grad: 15125.0089 LR: 5.00e-06  \n",
      "Epoch: [8][300/557] Elapsed 2m 6s (remain 1m 47s) Loss: 0.5988 Grad: 30127.6446 LR: 5.00e-06  \n",
      "Epoch: [8][400/557] Elapsed 2m 48s (remain 1m 5s) Loss: 0.5982 Grad: 10274.3954 LR: 5.00e-06  \n",
      "Epoch: [8][500/557] Elapsed 3m 30s (remain 0m 23s) Loss: 0.5981 Grad: 47544.1717 LR: 5.00e-06  \n",
      "Epoch: [8][556/557] Elapsed 3m 54s (remain 0m 0s) Loss: 0.5982 Grad: 32019.6958 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 4s) Loss: 0.6422 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.5982  avg_val_loss: 0.6550  time: 247s\n",
      "Epoch 8 - Score: 19.025511589317464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6550 \n",
      "Epoch: [9][0/557] Elapsed 0m 1s (remain 13m 42s) Loss: 0.5269 Grad: 6485.2670 LR: 5.00e-06  \n",
      "Epoch: [9][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.5798 Grad: 34584.5430 LR: 5.00e-06  \n",
      "Epoch: [9][200/557] Elapsed 1m 25s (remain 2m 30s) Loss: 0.5828 Grad: 11000.3885 LR: 5.00e-06  \n",
      "Epoch: [9][300/557] Elapsed 2m 6s (remain 1m 47s) Loss: 0.5824 Grad: 25344.9545 LR: 5.00e-06  \n",
      "Epoch: [9][400/557] Elapsed 2m 48s (remain 1m 5s) Loss: 0.5828 Grad: 8383.0932 LR: 5.00e-06  \n",
      "Epoch: [9][500/557] Elapsed 3m 29s (remain 0m 23s) Loss: 0.5840 Grad: 65164.5343 LR: 5.00e-06  \n",
      "Epoch: [9][556/557] Elapsed 3m 52s (remain 0m 0s) Loss: 0.5840 Grad: 24982.4732 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 8s) Loss: 0.6407 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.5840  avg_val_loss: 0.6558  time: 246s\n",
      "Epoch 9 - Score: 19.180821668968505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6558 \n",
      "Epoch: [10][0/557] Elapsed 0m 1s (remain 11m 20s) Loss: 0.5526 Grad: 5398.7176 LR: 5.00e-06  \n",
      "Epoch: [10][100/557] Elapsed 0m 43s (remain 3m 14s) Loss: 0.5741 Grad: 28009.9576 LR: 5.00e-06  \n",
      "Epoch: [10][200/557] Elapsed 1m 24s (remain 2m 28s) Loss: 0.5755 Grad: 7489.4800 LR: 5.00e-06  \n",
      "Epoch: [10][300/557] Elapsed 2m 5s (remain 1m 46s) Loss: 0.5755 Grad: 26298.6627 LR: 5.00e-06  \n",
      "Epoch: [10][400/557] Elapsed 2m 46s (remain 1m 4s) Loss: 0.5753 Grad: 17917.5991 LR: 5.00e-06  \n",
      "Epoch: [10][500/557] Elapsed 3m 27s (remain 0m 23s) Loss: 0.5753 Grad: 33214.4122 LR: 5.00e-06  \n",
      "Epoch: [10][556/557] Elapsed 3m 50s (remain 0m 0s) Loss: 0.5756 Grad: 34689.2288 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 16s) Loss: 0.6486 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.5756  avg_val_loss: 0.6628  time: 244s\n",
      "Epoch 10 - Score: 19.675621894307834\n",
      "========== fold: 0 result ==========\n",
      "Score: 17.63571\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6628 \n",
      "Epoch: [1][0/557] Elapsed 0m 1s (remain 12m 48s) Loss: 0.6845 Grad: 14540.3132 LR: 5.00e-06  \n",
      "Epoch: [1][100/557] Elapsed 0m 43s (remain 3m 15s) Loss: 0.6704 Grad: 27339.3004 LR: 5.00e-06  \n",
      "Epoch: [1][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6650 Grad: 9097.8322 LR: 5.00e-06  \n",
      "Epoch: [1][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.6600 Grad: 15847.4412 LR: 5.00e-06  \n",
      "Epoch: [1][400/557] Elapsed 2m 48s (remain 1m 5s) Loss: 0.6570 Grad: 9429.9286 LR: 5.00e-06  \n",
      "Epoch: [1][500/557] Elapsed 3m 29s (remain 0m 23s) Loss: 0.6557 Grad: 41631.8940 LR: 5.00e-06  \n",
      "Epoch: [1][556/557] Elapsed 3m 53s (remain 0m 0s) Loss: 0.6549 Grad: 25479.5751 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 20s) Loss: 0.5908 \n",
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6448 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.6549  avg_val_loss: 0.6448  time: 247s\n",
      "Epoch 1 - Score: 18.276543551650015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.644790).  Saving model ...\n",
      "Epoch: [2][0/557] Elapsed 0m 1s (remain 11m 55s) Loss: 0.6428 Grad: 12764.6929 LR: 5.00e-06  \n",
      "Epoch: [2][100/557] Elapsed 0m 43s (remain 3m 14s) Loss: 0.6414 Grad: 18796.4571 LR: 5.00e-06  \n",
      "Epoch: [2][200/557] Elapsed 1m 24s (remain 2m 29s) Loss: 0.6409 Grad: 7048.3440 LR: 5.00e-06  \n",
      "Epoch: [2][300/557] Elapsed 2m 5s (remain 1m 46s) Loss: 0.6405 Grad: 26231.2700 LR: 5.00e-06  \n",
      "Epoch: [2][400/557] Elapsed 2m 46s (remain 1m 4s) Loss: 0.6390 Grad: 8056.8576 LR: 5.00e-06  \n",
      "Epoch: [2][500/557] Elapsed 3m 27s (remain 0m 23s) Loss: 0.6388 Grad: 22421.8885 LR: 5.00e-06  \n",
      "Epoch: [2][556/557] Elapsed 3m 50s (remain 0m 0s) Loss: 0.6386 Grad: 27008.1448 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 7s) Loss: 0.5902 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.6386  avg_val_loss: 0.6419  time: 244s\n",
      "Epoch 2 - Score: 17.887342381356376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6419 \n",
      "Validation loss decreased (0.644790 --> 0.641878).  Saving model ...\n",
      "Epoch: [3][0/557] Elapsed 0m 1s (remain 12m 48s) Loss: 0.6751 Grad: 8730.6626 LR: 5.00e-06  \n",
      "Epoch: [3][100/557] Elapsed 0m 43s (remain 3m 17s) Loss: 0.6356 Grad: 94369.8051 LR: 5.00e-06  \n",
      "Epoch: [3][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6337 Grad: 16458.8204 LR: 5.00e-06  \n",
      "Epoch: [3][300/557] Elapsed 2m 8s (remain 1m 49s) Loss: 0.6343 Grad: 23695.1531 LR: 5.00e-06  \n",
      "Epoch: [3][400/557] Elapsed 2m 50s (remain 1m 6s) Loss: 0.6346 Grad: 20099.3271 LR: 5.00e-06  \n",
      "Epoch: [3][500/557] Elapsed 3m 31s (remain 0m 23s) Loss: 0.6343 Grad: 30598.0323 LR: 5.00e-06  \n",
      "Epoch: [3][556/557] Elapsed 3m 54s (remain 0m 0s) Loss: 0.6343 Grad: 38696.1513 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 7s) Loss: 0.5848 \n",
      "EVAL: [61/62] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6409 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.6343  avg_val_loss: 0.6409  time: 248s\n",
      "Epoch 3 - Score: 17.74200929978752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.641878 --> 0.640889).  Saving model ...\n",
      "Epoch: [4][0/557] Elapsed 0m 1s (remain 12m 29s) Loss: 0.6291 Grad: 12977.1866 LR: 5.00e-06  \n",
      "Epoch: [4][100/557] Elapsed 0m 43s (remain 3m 15s) Loss: 0.6247 Grad: 23183.7925 LR: 5.00e-06  \n",
      "Epoch: [4][200/557] Elapsed 1m 24s (remain 2m 30s) Loss: 0.6278 Grad: 13243.6153 LR: 5.00e-06  \n",
      "Epoch: [4][300/557] Elapsed 2m 6s (remain 1m 47s) Loss: 0.6269 Grad: 27880.8470 LR: 5.00e-06  \n",
      "Epoch: [4][400/557] Elapsed 2m 48s (remain 1m 5s) Loss: 0.6278 Grad: 14009.1560 LR: 5.00e-06  \n",
      "Epoch: [4][500/557] Elapsed 3m 30s (remain 0m 23s) Loss: 0.6273 Grad: 37116.8420 LR: 5.00e-06  \n",
      "Epoch: [4][556/557] Elapsed 3m 53s (remain 0m 0s) Loss: 0.6275 Grad: 45154.6167 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 5s) Loss: 0.5891 \n",
      "EVAL: [61/62] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6435 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.6275  avg_val_loss: 0.6435  time: 247s\n",
      "Epoch 4 - Score: 18.07647530953931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/557] Elapsed 0m 1s (remain 13m 59s) Loss: 0.5719 Grad: 17655.3586 LR: 5.00e-06  \n",
      "Epoch: [5][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6203 Grad: 47307.5888 LR: 5.00e-06  \n",
      "Epoch: [5][200/557] Elapsed 1m 24s (remain 2m 29s) Loss: 0.6190 Grad: 11503.9520 LR: 5.00e-06  \n",
      "Epoch: [5][300/557] Elapsed 2m 6s (remain 1m 47s) Loss: 0.6201 Grad: 30808.1959 LR: 5.00e-06  \n",
      "Epoch: [5][400/557] Elapsed 2m 47s (remain 1m 5s) Loss: 0.6197 Grad: 13181.9000 LR: 5.00e-06  \n",
      "Epoch: [5][500/557] Elapsed 3m 29s (remain 0m 23s) Loss: 0.6195 Grad: 45679.7932 LR: 5.00e-06  \n",
      "Epoch: [5][556/557] Elapsed 3m 52s (remain 0m 0s) Loss: 0.6195 Grad: 39826.5845 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 6s) Loss: 0.5835 \n",
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6473 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.6195  avg_val_loss: 0.6473  time: 246s\n",
      "Epoch 5 - Score: 18.486392245553535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/557] Elapsed 0m 1s (remain 12m 33s) Loss: 0.6147 Grad: 13118.8164 LR: 5.00e-06  \n",
      "Epoch: [6][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6140 Grad: 32993.6935 LR: 5.00e-06  \n",
      "Epoch: [6][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6130 Grad: 13923.2259 LR: 5.00e-06  \n",
      "Epoch: [6][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.6126 Grad: 43626.8297 LR: 5.00e-06  \n",
      "Epoch: [6][400/557] Elapsed 2m 49s (remain 1m 5s) Loss: 0.6125 Grad: 26018.0487 LR: 5.00e-06  \n",
      "Epoch: [6][500/557] Elapsed 3m 31s (remain 0m 23s) Loss: 0.6127 Grad: 43783.6144 LR: 5.00e-06  \n",
      "Epoch: [6][556/557] Elapsed 3m 55s (remain 0m 0s) Loss: 0.6125 Grad: 36828.4534 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 14s) Loss: 0.5818 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.6125  avg_val_loss: 0.6515  time: 249s\n",
      "Epoch 6 - Score: 18.91265609120814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6515 \n",
      "Epoch: [7][0/557] Elapsed 0m 1s (remain 12m 59s) Loss: 0.6008 Grad: 16386.4022 LR: 5.00e-06  \n",
      "Epoch: [7][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6057 Grad: 27889.8888 LR: 5.00e-06  \n",
      "Epoch: [7][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6063 Grad: 14632.8155 LR: 5.00e-06  \n",
      "Epoch: [7][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.6051 Grad: 54317.7982 LR: 5.00e-06  \n",
      "Epoch: [7][400/557] Elapsed 2m 50s (remain 1m 6s) Loss: 0.6051 Grad: 15840.4325 LR: 5.00e-06  \n",
      "Epoch: [7][500/557] Elapsed 3m 32s (remain 0m 23s) Loss: 0.6053 Grad: 50432.2982 LR: 5.00e-06  \n",
      "Epoch: [7][556/557] Elapsed 3m 56s (remain 0m 0s) Loss: 0.6051 Grad: 42562.5427 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 8s) Loss: 0.5843 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.6051  avg_val_loss: 0.6571  time: 250s\n",
      "Epoch 7 - Score: 19.498175241427603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6571 \n",
      "Epoch: [8][0/557] Elapsed 0m 1s (remain 12m 30s) Loss: 0.6378 Grad: 16282.1781 LR: 5.00e-06  \n",
      "Epoch: [8][100/557] Elapsed 0m 43s (remain 3m 17s) Loss: 0.6024 Grad: 35450.9039 LR: 5.00e-06  \n",
      "Epoch: [8][200/557] Elapsed 1m 25s (remain 2m 32s) Loss: 0.5966 Grad: 30285.6337 LR: 5.00e-06  \n",
      "Epoch: [8][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.5963 Grad: 48818.8773 LR: 5.00e-06  \n",
      "Epoch: [8][400/557] Elapsed 2m 49s (remain 1m 5s) Loss: 0.5966 Grad: 15471.1323 LR: 5.00e-06  \n",
      "Epoch: [8][500/557] Elapsed 3m 31s (remain 0m 23s) Loss: 0.5977 Grad: 39007.4962 LR: 5.00e-06  \n",
      "Epoch: [8][556/557] Elapsed 3m 54s (remain 0m 0s) Loss: 0.5972 Grad: 32751.1035 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 13s) Loss: 0.5779 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.5972  avg_val_loss: 0.6617  time: 248s\n",
      "Epoch 8 - Score: 19.90516683257135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6617 \n",
      "Epoch: [9][0/557] Elapsed 0m 1s (remain 13m 14s) Loss: 0.5500 Grad: 11472.2780 LR: 5.00e-06  \n",
      "Epoch: [9][100/557] Elapsed 0m 43s (remain 3m 15s) Loss: 0.5843 Grad: 22117.3110 LR: 5.00e-06  \n",
      "Epoch: [9][200/557] Elapsed 1m 25s (remain 2m 30s) Loss: 0.5814 Grad: 16014.5320 LR: 5.00e-06  \n",
      "Epoch: [9][300/557] Elapsed 2m 6s (remain 1m 47s) Loss: 0.5798 Grad: 41266.0398 LR: 5.00e-06  \n",
      "Epoch: [9][400/557] Elapsed 2m 48s (remain 1m 5s) Loss: 0.5816 Grad: 9350.8312 LR: 5.00e-06  \n",
      "Epoch: [9][500/557] Elapsed 3m 30s (remain 0m 23s) Loss: 0.5827 Grad: 28348.8898 LR: 5.00e-06  \n",
      "Epoch: [9][556/557] Elapsed 3m 54s (remain 0m 0s) Loss: 0.5829 Grad: 47509.8663 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 11s) Loss: 0.5816 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.5829  avg_val_loss: 0.6552  time: 248s\n",
      "Epoch 9 - Score: 19.313497119159393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6552 \n",
      "Epoch: [10][0/557] Elapsed 0m 1s (remain 13m 17s) Loss: 0.5712 Grad: 9871.6279 LR: 5.00e-06  \n",
      "Epoch: [10][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.5747 Grad: 20540.2657 LR: 5.00e-06  \n",
      "Epoch: [10][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.5756 Grad: 9269.7123 LR: 5.00e-06  \n",
      "Epoch: [10][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.5762 Grad: 36770.6148 LR: 5.00e-06  \n",
      "Epoch: [10][400/557] Elapsed 2m 49s (remain 1m 5s) Loss: 0.5751 Grad: 14823.7450 LR: 5.00e-06  \n",
      "Epoch: [10][500/557] Elapsed 3m 31s (remain 0m 23s) Loss: 0.5755 Grad: 19582.2728 LR: 5.00e-06  \n",
      "Epoch: [10][556/557] Elapsed 3m 54s (remain 0m 0s) Loss: 0.5751 Grad: 22813.6855 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 13s) Loss: 0.5849 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.5751  avg_val_loss: 0.6635  time: 248s\n",
      "Epoch 10 - Score: 20.022712824134437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6635 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 result ==========\n",
      "Score: 17.74201\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/557] Elapsed 0m 1s (remain 12m 39s) Loss: 0.6905 Grad: 11324.5382 LR: 5.00e-06  \n",
      "Epoch: [1][100/557] Elapsed 0m 43s (remain 3m 17s) Loss: 0.6713 Grad: 28443.4119 LR: 5.00e-06  \n",
      "Epoch: [1][200/557] Elapsed 1m 26s (remain 2m 32s) Loss: 0.6632 Grad: 6155.2022 LR: 5.00e-06  \n",
      "Epoch: [1][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.6587 Grad: 23037.1104 LR: 5.00e-06  \n",
      "Epoch: [1][400/557] Elapsed 2m 49s (remain 1m 6s) Loss: 0.6558 Grad: 9058.0423 LR: 5.00e-06  \n",
      "Epoch: [1][500/557] Elapsed 3m 31s (remain 0m 23s) Loss: 0.6546 Grad: 24180.2239 LR: 5.00e-06  \n",
      "Epoch: [1][556/557] Elapsed 3m 55s (remain 0m 0s) Loss: 0.6538 Grad: 17567.8318 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 10s) Loss: 0.6413 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.6538  avg_val_loss: 0.6457  time: 250s\n",
      "Epoch 1 - Score: 18.472917000930206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 14s (remain 0m 0s) Loss: 0.6457 \n",
      "Validation loss decreased (inf --> 0.645732).  Saving model ...\n",
      "Epoch: [2][0/557] Elapsed 0m 1s (remain 15m 37s) Loss: 0.6526 Grad: 15593.7292 LR: 5.00e-06  \n",
      "Epoch: [2][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6426 Grad: 16804.4411 LR: 5.00e-06  \n",
      "Epoch: [2][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6408 Grad: 9506.5967 LR: 5.00e-06  \n",
      "Epoch: [2][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.6401 Grad: 24410.8998 LR: 5.00e-06  \n",
      "Epoch: [2][400/557] Elapsed 2m 49s (remain 1m 5s) Loss: 0.6395 Grad: 9171.1407 LR: 5.00e-06  \n",
      "Epoch: [2][500/557] Elapsed 3m 30s (remain 0m 23s) Loss: 0.6389 Grad: 22159.6975 LR: 5.00e-06  \n",
      "Epoch: [2][556/557] Elapsed 3m 54s (remain 0m 0s) Loss: 0.6386 Grad: 26705.2616 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 5s) Loss: 0.6464 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.6386  avg_val_loss: 0.6430  time: 247s\n",
      "Epoch 2 - Score: 18.114685123556935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6430 \n",
      "Validation loss decreased (0.645732 --> 0.642983).  Saving model ...\n",
      "Epoch: [3][0/557] Elapsed 0m 1s (remain 14m 13s) Loss: 0.5954 Grad: 17466.2675 LR: 5.00e-06  \n",
      "Epoch: [3][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6324 Grad: 21511.6292 LR: 5.00e-06  \n",
      "Epoch: [3][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6355 Grad: 13179.7718 LR: 5.00e-06  \n",
      "Epoch: [3][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.6348 Grad: 23168.3625 LR: 5.00e-06  \n",
      "Epoch: [3][400/557] Elapsed 2m 49s (remain 1m 5s) Loss: 0.6346 Grad: 10601.5175 LR: 5.00e-06  \n",
      "Epoch: [3][500/557] Elapsed 3m 31s (remain 0m 23s) Loss: 0.6344 Grad: 35319.1153 LR: 5.00e-06  \n",
      "Epoch: [3][556/557] Elapsed 3m 55s (remain 0m 0s) Loss: 0.6340 Grad: 45919.5226 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 9s) Loss: 0.6499 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.6340  avg_val_loss: 0.6441  time: 249s\n",
      "Epoch 3 - Score: 18.252560625797518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6441 \n",
      "Epoch: [4][0/557] Elapsed 0m 1s (remain 13m 0s) Loss: 0.6461 Grad: 11683.8576 LR: 5.00e-06  \n",
      "Epoch: [4][100/557] Elapsed 0m 44s (remain 3m 18s) Loss: 0.6298 Grad: 19082.9319 LR: 5.00e-06  \n",
      "Epoch: [4][200/557] Elapsed 1m 26s (remain 2m 33s) Loss: 0.6291 Grad: 14438.9592 LR: 5.00e-06  \n",
      "Epoch: [4][300/557] Elapsed 2m 9s (remain 1m 50s) Loss: 0.6300 Grad: 25501.0202 LR: 5.00e-06  \n",
      "Epoch: [4][400/557] Elapsed 2m 51s (remain 1m 6s) Loss: 0.6290 Grad: 10832.6214 LR: 5.00e-06  \n",
      "Epoch: [4][500/557] Elapsed 3m 33s (remain 0m 23s) Loss: 0.6277 Grad: 37701.5764 LR: 5.00e-06  \n",
      "Epoch: [4][556/557] Elapsed 3m 57s (remain 0m 0s) Loss: 0.6279 Grad: 39626.7303 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 10s) Loss: 0.6411 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.6279  avg_val_loss: 0.6457  time: 251s\n",
      "Epoch 4 - Score: 18.449913939860107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6457 \n",
      "Epoch: [5][0/557] Elapsed 0m 1s (remain 13m 17s) Loss: 0.6046 Grad: 11557.2166 LR: 5.00e-06  \n",
      "Epoch: [5][100/557] Elapsed 0m 43s (remain 3m 15s) Loss: 0.6224 Grad: 25905.7805 LR: 5.00e-06  \n",
      "Epoch: [5][200/557] Elapsed 1m 25s (remain 2m 30s) Loss: 0.6211 Grad: 13995.4637 LR: 5.00e-06  \n",
      "Epoch: [5][300/557] Elapsed 2m 6s (remain 1m 47s) Loss: 0.6216 Grad: 36560.4225 LR: 5.00e-06  \n",
      "Epoch: [5][400/557] Elapsed 2m 48s (remain 1m 5s) Loss: 0.6206 Grad: 32525.5817 LR: 5.00e-06  \n",
      "Epoch: [5][500/557] Elapsed 3m 30s (remain 0m 23s) Loss: 0.6209 Grad: 32881.8033 LR: 5.00e-06  \n",
      "Epoch: [5][556/557] Elapsed 3m 53s (remain 0m 0s) Loss: 0.6214 Grad: 54450.2387 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 8s) Loss: 0.6596 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.6214  avg_val_loss: 0.6474  time: 247s\n",
      "Epoch 5 - Score: 18.676400449694036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6474 \n",
      "Epoch: [6][0/557] Elapsed 0m 1s (remain 13m 29s) Loss: 0.5793 Grad: 15874.6129 LR: 5.00e-06  \n",
      "Epoch: [6][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6159 Grad: 31906.0225 LR: 5.00e-06  \n",
      "Epoch: [6][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6142 Grad: 11324.8161 LR: 5.00e-06  \n",
      "Epoch: [6][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.6144 Grad: 43985.4085 LR: 5.00e-06  \n",
      "Epoch: [6][400/557] Elapsed 2m 49s (remain 1m 5s) Loss: 0.6134 Grad: 12744.1661 LR: 5.00e-06  \n",
      "Epoch: [6][500/557] Elapsed 3m 31s (remain 0m 23s) Loss: 0.6131 Grad: 52995.3547 LR: 5.00e-06  \n",
      "Epoch: [6][556/557] Elapsed 3m 55s (remain 0m 0s) Loss: 0.6141 Grad: 32322.2175 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 6s) Loss: 0.6708 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.6141  avg_val_loss: 0.6519  time: 248s\n",
      "Epoch 6 - Score: 19.129768518241065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6519 \n",
      "Epoch: [7][0/557] Elapsed 0m 1s (remain 14m 36s) Loss: 0.6159 Grad: 14951.9979 LR: 5.00e-06  \n",
      "Epoch: [7][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6063 Grad: 34264.4931 LR: 5.00e-06  \n",
      "Epoch: [7][200/557] Elapsed 1m 25s (remain 2m 32s) Loss: 0.6062 Grad: 19370.9861 LR: 5.00e-06  \n",
      "Epoch: [7][300/557] Elapsed 2m 8s (remain 1m 48s) Loss: 0.6072 Grad: 54985.9390 LR: 5.00e-06  \n",
      "Epoch: [7][400/557] Elapsed 2m 50s (remain 1m 6s) Loss: 0.6058 Grad: 18996.5090 LR: 5.00e-06  \n",
      "Epoch: [7][500/557] Elapsed 3m 32s (remain 0m 23s) Loss: 0.6060 Grad: 32653.9500 LR: 5.00e-06  \n",
      "Epoch: [7][556/557] Elapsed 3m 56s (remain 0m 0s) Loss: 0.6062 Grad: 40105.2362 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 13s) Loss: 0.6669 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.6062  avg_val_loss: 0.6559  time: 250s\n",
      "Epoch 7 - Score: 19.568512148528768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6559 \n",
      "Epoch: [8][0/557] Elapsed 0m 1s (remain 12m 24s) Loss: 0.6048 Grad: 16904.7633 LR: 5.00e-06  \n",
      "Epoch: [8][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6005 Grad: 39065.9707 LR: 5.00e-06  \n",
      "Epoch: [8][200/557] Elapsed 1m 25s (remain 2m 32s) Loss: 0.6002 Grad: 18245.0463 LR: 5.00e-06  \n",
      "Epoch: [8][300/557] Elapsed 2m 8s (remain 1m 48s) Loss: 0.5994 Grad: 51307.8617 LR: 5.00e-06  \n",
      "Epoch: [8][400/557] Elapsed 2m 51s (remain 1m 6s) Loss: 0.5981 Grad: 19718.9214 LR: 5.00e-06  \n",
      "Epoch: [8][500/557] Elapsed 3m 33s (remain 0m 23s) Loss: 0.5994 Grad: 50119.2471 LR: 5.00e-06  \n",
      "Epoch: [8][556/557] Elapsed 3m 57s (remain 0m 0s) Loss: 0.5996 Grad: 46509.8162 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 4s) Loss: 0.6562 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.5996  avg_val_loss: 0.6600  time: 251s\n",
      "Epoch 8 - Score: 19.98828579835172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6600 \n",
      "Epoch: [9][0/557] Elapsed 0m 1s (remain 12m 15s) Loss: 0.5589 Grad: 17448.5878 LR: 5.00e-06  \n",
      "Epoch: [9][100/557] Elapsed 0m 43s (remain 3m 17s) Loss: 0.5869 Grad: 21517.5940 LR: 5.00e-06  \n",
      "Epoch: [9][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.5862 Grad: 12927.8223 LR: 5.00e-06  \n",
      "Epoch: [9][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.5846 Grad: 33807.9941 LR: 5.00e-06  \n",
      "Epoch: [9][400/557] Elapsed 2m 49s (remain 1m 5s) Loss: 0.5837 Grad: 11022.8127 LR: 5.00e-06  \n",
      "Epoch: [9][500/557] Elapsed 3m 31s (remain 0m 23s) Loss: 0.5851 Grad: 45716.8113 LR: 5.00e-06  \n",
      "Epoch: [9][556/557] Elapsed 3m 54s (remain 0m 0s) Loss: 0.5851 Grad: 25594.8497 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6756 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.5851  avg_val_loss: 0.6604  time: 248s\n",
      "Epoch 9 - Score: 19.989233469094845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6604 \n",
      "Epoch: [10][0/557] Elapsed 0m 1s (remain 12m 32s) Loss: 0.5498 Grad: 9691.1904 LR: 5.00e-06  \n",
      "Epoch: [10][100/557] Elapsed 0m 43s (remain 3m 14s) Loss: 0.5754 Grad: 21608.9569 LR: 5.00e-06  \n",
      "Epoch: [10][200/557] Elapsed 1m 24s (remain 2m 30s) Loss: 0.5734 Grad: 10318.7813 LR: 5.00e-06  \n",
      "Epoch: [10][300/557] Elapsed 2m 6s (remain 1m 47s) Loss: 0.5748 Grad: 25527.3615 LR: 5.00e-06  \n",
      "Epoch: [10][400/557] Elapsed 2m 47s (remain 1m 5s) Loss: 0.5757 Grad: 13387.1727 LR: 5.00e-06  \n",
      "Epoch: [10][500/557] Elapsed 3m 29s (remain 0m 23s) Loss: 0.5769 Grad: 20522.2502 LR: 5.00e-06  \n",
      "Epoch: [10][556/557] Elapsed 3m 52s (remain 0m 0s) Loss: 0.5772 Grad: 21471.4830 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 9s) Loss: 0.6589 \n",
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6642 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.5772  avg_val_loss: 0.6642  time: 246s\n",
      "Epoch 10 - Score: 20.323277428159614\n",
      "========== fold: 2 result ==========\n",
      "Score: 18.11469\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/557] Elapsed 0m 1s (remain 12m 23s) Loss: 0.7117 Grad: 20543.6753 LR: 5.00e-06  \n",
      "Epoch: [1][100/557] Elapsed 0m 43s (remain 3m 15s) Loss: 0.6774 Grad: 28317.4083 LR: 5.00e-06  \n",
      "Epoch: [1][200/557] Elapsed 1m 25s (remain 2m 30s) Loss: 0.6675 Grad: 8174.7712 LR: 5.00e-06  \n",
      "Epoch: [1][300/557] Elapsed 2m 6s (remain 1m 47s) Loss: 0.6616 Grad: 15084.4914 LR: 5.00e-06  \n",
      "Epoch: [1][400/557] Elapsed 2m 48s (remain 1m 5s) Loss: 0.6591 Grad: 9546.6315 LR: 5.00e-06  \n",
      "Epoch: [1][500/557] Elapsed 3m 30s (remain 0m 23s) Loss: 0.6573 Grad: 16515.4898 LR: 5.00e-06  \n",
      "Epoch: [1][556/557] Elapsed 3m 53s (remain 0m 0s) Loss: 0.6565 Grad: 19145.2869 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 18s) Loss: 0.6145 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.6565  avg_val_loss: 0.6451  time: 247s\n",
      "Epoch 1 - Score: 18.17729248227026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6451 \n",
      "Validation loss decreased (inf --> 0.645130).  Saving model ...\n",
      "Epoch: [2][0/557] Elapsed 0m 1s (remain 12m 42s) Loss: 0.6343 Grad: 3980.4321 LR: 5.00e-06  \n",
      "Epoch: [2][100/557] Elapsed 0m 43s (remain 3m 18s) Loss: 0.6420 Grad: 20216.1416 LR: 5.00e-06  \n",
      "Epoch: [2][200/557] Elapsed 1m 26s (remain 2m 33s) Loss: 0.6407 Grad: 13031.2297 LR: 5.00e-06  \n",
      "Epoch: [2][300/557] Elapsed 2m 9s (remain 1m 50s) Loss: 0.6404 Grad: 28747.0541 LR: 5.00e-06  \n",
      "Epoch: [2][400/557] Elapsed 2m 51s (remain 1m 6s) Loss: 0.6404 Grad: 6648.9608 LR: 5.00e-06  \n",
      "Epoch: [2][500/557] Elapsed 3m 33s (remain 0m 23s) Loss: 0.6398 Grad: 17587.0590 LR: 5.00e-06  \n",
      "Epoch: [2][556/557] Elapsed 3m 57s (remain 0m 0s) Loss: 0.6396 Grad: 32356.8633 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 12s) Loss: 0.6106 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.6396  avg_val_loss: 0.6404  time: 251s\n",
      "Epoch 2 - Score: 17.54897442064487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6404 \n",
      "Validation loss decreased (0.645130 --> 0.640373).  Saving model ...\n",
      "Epoch: [3][0/557] Elapsed 0m 1s (remain 13m 3s) Loss: 0.6072 Grad: 26592.8469 LR: 5.00e-06  \n",
      "Epoch: [3][100/557] Elapsed 0m 43s (remain 3m 18s) Loss: 0.6310 Grad: 23575.3503 LR: 5.00e-06  \n",
      "Epoch: [3][200/557] Elapsed 1m 26s (remain 2m 32s) Loss: 0.6334 Grad: 20478.7641 LR: 5.00e-06  \n",
      "Epoch: [3][300/557] Elapsed 2m 8s (remain 1m 49s) Loss: 0.6348 Grad: 50975.2156 LR: 5.00e-06  \n",
      "Epoch: [3][400/557] Elapsed 2m 50s (remain 1m 6s) Loss: 0.6352 Grad: 10311.4917 LR: 5.00e-06  \n",
      "Epoch: [3][500/557] Elapsed 3m 33s (remain 0m 23s) Loss: 0.6353 Grad: 32427.3585 LR: 5.00e-06  \n",
      "Epoch: [3][556/557] Elapsed 3m 56s (remain 0m 0s) Loss: 0.6352 Grad: 34207.3824 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 6s) Loss: 0.6091 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.6352  avg_val_loss: 0.6390  time: 250s\n",
      "Epoch 3 - Score: 17.376353674767884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6390 \n",
      "Validation loss decreased (0.640373 --> 0.639038).  Saving model ...\n",
      "Epoch: [4][0/557] Elapsed 0m 1s (remain 17m 15s) Loss: 0.6434 Grad: 8222.7478 LR: 5.00e-06  \n",
      "Epoch: [4][100/557] Elapsed 0m 44s (remain 3m 20s) Loss: 0.6313 Grad: 21162.9846 LR: 5.00e-06  \n",
      "Epoch: [4][200/557] Elapsed 1m 26s (remain 2m 33s) Loss: 0.6293 Grad: 13869.9114 LR: 5.00e-06  \n",
      "Epoch: [4][300/557] Elapsed 2m 9s (remain 1m 49s) Loss: 0.6293 Grad: 35625.6504 LR: 5.00e-06  \n",
      "Epoch: [4][400/557] Elapsed 2m 53s (remain 1m 7s) Loss: 0.6295 Grad: 11546.3521 LR: 5.00e-06  \n",
      "Epoch: [4][500/557] Elapsed 3m 39s (remain 0m 24s) Loss: 0.6295 Grad: 32845.5269 LR: 5.00e-06  \n",
      "Epoch: [4][556/557] Elapsed 4m 5s (remain 0m 0s) Loss: 0.6292 Grad: 26801.4711 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 2s) Loss: 0.6014 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.6292  avg_val_loss: 0.6407  time: 262s\n",
      "Epoch 4 - Score: 17.54651917054564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 15s (remain 0m 0s) Loss: 0.6407 \n",
      "Epoch: [5][0/557] Elapsed 0m 1s (remain 12m 17s) Loss: 0.6145 Grad: 6269.0300 LR: 5.00e-06  \n",
      "Epoch: [5][100/557] Elapsed 0m 46s (remain 3m 30s) Loss: 0.6237 Grad: 17227.7211 LR: 5.00e-06  \n",
      "Epoch: [5][200/557] Elapsed 1m 31s (remain 2m 42s) Loss: 0.6232 Grad: 13341.7543 LR: 5.00e-06  \n",
      "Epoch: [5][300/557] Elapsed 2m 18s (remain 1m 57s) Loss: 0.6208 Grad: 34196.8026 LR: 5.00e-06  \n",
      "Epoch: [5][400/557] Elapsed 3m 4s (remain 1m 11s) Loss: 0.6219 Grad: 10664.8749 LR: 5.00e-06  \n",
      "Epoch: [5][500/557] Elapsed 3m 50s (remain 0m 25s) Loss: 0.6223 Grad: 31488.2743 LR: 5.00e-06  \n",
      "Epoch: [5][556/557] Elapsed 4m 13s (remain 0m 0s) Loss: 0.6228 Grad: 27008.8106 LR: 5.00e-06  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 15s) Loss: 0.6069 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.6228  avg_val_loss: 0.6401  time: 267s\n",
      "Epoch 5 - Score: 17.507731967952502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6401 \n",
      "Epoch: [6][0/557] Elapsed 0m 1s (remain 12m 51s) Loss: 0.6005 Grad: 27728.6706 LR: 5.00e-06  \n",
      "Epoch: [6][100/557] Elapsed 0m 43s (remain 3m 16s) Loss: 0.6181 Grad: 28527.4112 LR: 5.00e-06  \n",
      "Epoch: [6][200/557] Elapsed 1m 25s (remain 2m 31s) Loss: 0.6168 Grad: 5201.9914 LR: 5.00e-06  \n",
      "Epoch: [6][300/557] Elapsed 2m 7s (remain 1m 48s) Loss: 0.6175 Grad: 29735.2337 LR: 5.00e-06  \n",
      "Epoch: [6][400/557] Elapsed 2m 50s (remain 1m 6s) Loss: 0.6167 Grad: 17171.3543 LR: 5.00e-06  \n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ijJJUa2d2Lx"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "petfinder2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py38-all-in-one-dev",
   "language": "python",
   "name": "py38-all-in-one-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
